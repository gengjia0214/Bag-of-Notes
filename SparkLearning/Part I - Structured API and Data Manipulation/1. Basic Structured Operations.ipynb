{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Learning Note - Basic Structured Operations\n",
    "\n",
    "Jia Geng | gjia0214@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** \n",
    "- some functions in spark 2.4 are not well supported by java 11, use java 8!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='directory'></a>\n",
    "\n",
    "## Directory \n",
    "\n",
    "- [Data Source](https://github.com/databricks/Spark-The-Definitive-Guide/tree/master/data/flight-data/csv)\n",
    "- [0. Spark Session](#sec0)\n",
    "- [1. Read File into DataFrame](#sec1)\n",
    "- [2. Load File with Manual Schema ](#sec2)\n",
    "- [3. Column, Row, and Create DataFrame from Scratch](#sec3)\n",
    "- [4. Select & SelectExpr ](#sec4)\n",
    "- [5. Column Manipulation](#sec5)\n",
    "- [6. Row Manipulation](#sec6)\n",
    "- [7. Partitions and Collect](#sec7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame vs Dataset\n",
    "\n",
    "- DataFrames are untyped until runtime\n",
    "- Datasets are typed during the compiling time\n",
    "- Usually just work with DataFrames. When need strict compile-time checking, Dataset is prefered.\n",
    "- Since Python is Dynamic language, it does not support Datasets (at least for Spark2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured API Execution Steps\n",
    "\n",
    "*NOTE: DataFrame is immutable! All APIs will return a new DataFrame!*\n",
    "\n",
    "- Write DataFrame/Dataset/SQL code\n",
    "- Spark convert code to logical plan (analyze and **optimize** the logical plan)\n",
    "- Spark transform logical plan to physical plan (**optimize** the physical plan, how to execute on cluster)\n",
    "- Spark Execute physical plan on clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"1.8.0_252\"\r\n",
      "OpenJDK Runtime Environment (build 1.8.0_252-8u252-b09-1~19.10-b09)\r\n",
      "OpenJDK 64-Bit Server VM (build 25.252-b09, mixed mode)\r\n"
     ]
    }
   ],
   "source": [
    "# check java version \n",
    "# use sudo update-alternatives --config java to switch java version if needed.\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "data_example_path = '/home/jgeng/Documents/Git/SparkLearning/data/2015flight.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Spark Session <a id='sec0'></a>\n",
    "The entry point into all functionality in Spark is the SparkSession class. To create a basic SparkSession, just use SparkSession.builder:\n",
    "\n",
    "\n",
    "[back to top](#directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://unknown40A5EF2BBD8A:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff9b05e51d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a spark session locally\n",
    "spark = SparkSession.builder.appName('Spark Example').getOrCreate()\n",
    "\n",
    "# specify the number of worker\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read File into DataFrame  <a id='sec1'></a>\n",
    "\n",
    "Spark session can read file of differemt formats.\n",
    "- Use `.format()` to specify file format\n",
    "- `option()` provide many configurations for reading the data such as read header\n",
    "- DataFrame object has a method `printSchema()`\n",
    "\n",
    "DataType on Read:\n",
    "- `json` file contains information regarding the type of the data (but not precision). \n",
    "- data in `csv` file will be read as string if not specified\n",
    "- **the** `option('inferSchema', True)` **will enable the schema inference for reading the csv file**\n",
    "\n",
    "For faster access of data, use `df.cache()` to put the data in memory\n",
    "When to use caching: As suggested in this post, it is recommended to use caching in the following situations:\n",
    "\n",
    "- RDD re-use in iterative machine learning applications\n",
    "- RDD re-use in standalone Spark applications\n",
    "- When RDD computation is expensive, caching can help in reducing the cost of recovery in the case one executor fails\n",
    "\n",
    "**`df.cache() is lazy operation, it does not cache the data until you use it!`**\n",
    "\n",
    "[*back to top*](#directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read from csv file\n",
    "df = spark.read.format('csv').load(data_example_path)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# very handy method\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|              _c0|                _c1|  _c2|\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# when does not specify the header, spark will treate each row as a data record and add header\n",
    "df.show(3)\n",
    "df.printSchema() # count is in StringType!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use option to read header\n",
    "df = spark.read.format('csv').option('header', True).option('inferSchema', True).load(data_example_path)\n",
    "df.show(3)\n",
    "df.printSchema()  # now count is in IntegerType!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache the dataframe\n",
    "df.cache() # cache it on memory if the table will be access frequently\n",
    "df.createOrReplaceTempView('dfTable')  # this is for running the sql code on it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load File with Manual Schema <a id='sec2'></a>\n",
    "\n",
    "When read data from file, if not specifying the schema of your data, default schema-on-read will be created for the data. **This could cause precision issue if the data in file is in different precision. In production, it is usually recommended to manually setup the schema.**\n",
    "\n",
    "`pyspark.sql.types` have all supported data types. \n",
    "\n",
    "For a list of all: https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/ch04.html\n",
    "\n",
    "Spark DataFrame Schema is defiend by StructType and a list of StructField within it.\n",
    "\n",
    "StructField:\n",
    "- name: name of the field/column\n",
    "- type: data type\n",
    "- nullable: whether null value is allowed\n",
    "- metadata: way of storing information about this column (will be used in machine learning)\n",
    "\n",
    "[back to top](#directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,IntegerType,true)))\n"
     ]
    }
   ],
   "source": [
    "# df have a schema attribute\n",
    "print(df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify a schema manually - e.g. what if count is long rather than integer\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "\n",
    "# field name, data type, nullable\n",
    "field_1 = StructField('DEST_COUNTRY_NAME', StringType(), True)\n",
    "field_2 = StructField('ORIGIN_COUNTRY_NAME', StringType(), True)\n",
    "field_3 = StructField('count', LongType(), False)\n",
    "\n",
    "manualSchema = StructType([field_1, field_2, field_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('csv').option('header', True).schema(manualSchema).load(data_example_path)\n",
    "df.show(3) \n",
    "\n",
    "# now count is long! \n",
    "# but nullable is true\n",
    "# this is because CSV format doesn't provide any tools which allow you to specify data constraints \n",
    "# so by definition reader cannot assume that input is not null and your data indeed contains nulls.\n",
    "df.printSchema()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Column, Row, and Create DataFrame from Scratch  <a id='sec3'></a>\n",
    "\n",
    "col can be used in expression for `select()` method.\n",
    "\n",
    "row is data records.\n",
    "\n",
    "[back to top](#directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'new_col'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# create an unattached column\n",
    "new_col = col('new_col')\n",
    "new_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'count'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a column from df\n",
    "df['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DEST_COUNTRY_NAME'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all column names from a df\n",
    "df.columns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# create a new data record\n",
    "newRow = Row('Hi', None, 2, True)\n",
    "\n",
    "# access a value from the data record\n",
    "newRow[0]  # Python will automatically convert the value to correct type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|              NYC|                MIA|    2|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a DataFrame from Scratch\n",
    "# field name, data type, nullable\n",
    "field_1 = StructField('DEST_COUNTRY_NAME', StringType(), True)\n",
    "field_2 = StructField('ORIGIN_COUNTRY_NAME', StringType(), True)\n",
    "field_3 = StructField('count', LongType(), False)\n",
    "\n",
    "manualSchema = StructType([field_1, field_2, field_3])\n",
    "\n",
    "newRow = Row('NYC', 'MIA', 2)\n",
    "\n",
    "newDF = spark.createDataFrame([newRow], manualSchema)\n",
    "newDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Select & SelectExpr <a id='sec4'></a>\n",
    " \n",
    "Flexible expression on columns data.\n",
    "- use `expr()` in methods such as `select()` and `withColumn()`\n",
    "- use `selectExpr()` instead of `select(expr(..), expr(..), ..)`\n",
    "\n",
    "[back to top](#directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|deST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "|    United States|\n",
      "|            Egypt|\n",
      "+-----------------+\n",
      "only showing top 4 rows\n",
      "\n",
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "|    United States|\n",
      "|            Egypt|\n",
      "+-----------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# by default the spark session is case insensitive\n",
    "spark.sql('set spark.sql.caseSensitive=false')\n",
    "df.select('deST_COUNTRY_NAME').show(4)  # case insensitive!\n",
    "\n",
    "# change the current session to be case sensitive\n",
    "spark.sql('set spark.sql.caseSensitive=true')\n",
    "df.select('DEST_COUNTRY_NAME').show(4)\n",
    "\n",
    "# change it back\n",
    "spark.sql('set spark.sql.caseSensitive=false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|  destination|    departure|\n",
      "+-------------+-------------+\n",
      "|United States|      Romania|\n",
      "|United States|      Croatia|\n",
      "|United States|      Ireland|\n",
      "|        Egypt|United States|\n",
      "+-------------+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# use AS to rename the column name\n",
    "# the returned new DataFrame will have a new column name\n",
    "newdf = df.select(expr(\"DEST_COUNTRY_NAME as destination\"), \n",
    "          expr('ORIGIN_COUNTRY_NAME as departure'))\n",
    "newdf.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+-------+\n",
      "|  destination|    departure|invalid|\n",
      "+-------------+-------------+-------+\n",
      "|United States|      Romania|  false|\n",
      "|United States|      Croatia|  false|\n",
      "|United States|      Ireland|  false|\n",
      "|        Egypt|United States|  false|\n",
      "+-------------+-------------+-------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use * to select all column\n",
    "# expr can take some more operations between columns to create new column\n",
    "newdf.select('*', expr('destination=departure as invalid')).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------+---+----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|invalid|one|bool|\n",
      "+-----------------+-------------------+-----+-------+---+----+\n",
      "|    United States|            Romania|   15|  false|  1|true|\n",
      "|    United States|            Croatia|    1|  false|  1|true|\n",
      "|    United States|            Ireland|  344|  false|  1|true|\n",
      "|            Egypt|      United States|   15|  false|  1|true|\n",
      "+-----------------+-------------------+-----+-------+---+----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use selectExpr to do it all use one line\n",
    "# takes the expression as input, it can also take a literal as input1\n",
    "df.selectExpr('*', 'DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as invalid', '1 as one', 'True as bool').show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Column Manipulation <a id='sec5'></a>\n",
    "For maniplating column\n",
    "- use `df.withColumn()` to add column, change column type.\n",
    "    - this method takes two params: `column name`, `expression`\n",
    "    - if column name does not exist, it will append a new column\n",
    "    - if column name exist, it will replace the column with new expression result\n",
    "- use `df.withColumnRenamed()` to rename a column\n",
    "- column name should avoid reserved characters and keywords such as `as`. If needed, use \\`...\\` to skip.\n",
    "- use `df.drop()` to drop a column\n",
    "\n",
    "Spark session is by default case insensitive.\n",
    "- use `spark_session.sql('set spark.sql.caseSensitive=false')` to change to case sensitive\n",
    "\n",
    "Add an id column:\n",
    "- `monotonically_increasing_id()` return an id column\n",
    "\n",
    "[back to top](#directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|one|\n",
      "+-----------------+-------------------+-----+---+\n",
      "|    United States|            Romania|   15|  1|\n",
      "|    United States|            Croatia|    1|  1|\n",
      "+-----------------+-------------------+-----+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# a more formal way to add column is to use withColumn\n",
    "# must use lit to create a column of 1, otherwise, it will break\n",
    "df.withColumn('one', lit(1)).show(2)  # this is NOT inplace, scala never inplace change table!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|invalid|\n",
      "+-----------------+-------------------+-----+-------+\n",
      "|    United States|            Romania|   15|  false|\n",
      "|    United States|            Croatia|    1|  false|\n",
      "+-----------------+-------------------+-----+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# withcolumn also support expr\n",
    "df.withColumn('invalid', expr('DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|            false|            Romania|   15|\n",
      "|            false|            Croatia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|  destination|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|United States|\n",
      "|    United States|            Croatia|    1|United States|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# when column name exist, it will replace the column\n",
    "df.withColumn('DEST_COUNTRY_NAME', expr('DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME')).show(2)\n",
    "\n",
    "# withColumn can also be used for copy and rename a column\n",
    "df.withColumn('destination', expr('DEST_COUNTRY_NAME')).show(2)\n",
    "\n",
    "# withColumn can also be used for change the type of the column\n",
    "# use col('count') to get column type and cast() to cast into new type\n",
    "df.withColumn('count', col('count').cast('long')).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+-----+\n",
      "|         dest|ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------+-------------------+-----+\n",
      "|United States|            Romania|   15|\n",
      "|United States|            Croatia|    1|\n",
      "+-------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# a more straight forward way to rename a column is \n",
    "df.withColumnRenamed('DEST_COUNTRY_NAME', 'dest').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
      "+-----------------+-------------------+\n",
      "|    United States|            Romania|\n",
      "|    United States|            Croatia|\n",
      "+-----------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------------------+\n",
      "|ORIGIN_COUNTRY_NAME|\n",
      "+-------------------+\n",
      "|            Romania|\n",
      "|            Croatia|\n",
      "+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to drop a column\n",
    "df.drop('count').show(2)\n",
    "\n",
    "# drop multiple column at a time\n",
    "df.drop('count', 'DEST_COUNTRY_NAME').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Row Manipulation <a id='sec6'></a>\n",
    "\n",
    "Basic Manipluation including:\n",
    "- Filtering: filter rows with some expression with T/F output: `df.filter()` or `df.where()`\n",
    "    - note that `df.select(condition)` will return a table with single column of true/false.\n",
    "- Get Unique: `df.distinct()`\n",
    "- Random Samples: `df.sample(withReplacement=, fraction=, seed=)`\n",
    "- Random Splits: `df.randomSplit(fractions=, seed=)`\n",
    "- Concat and Append: `df.union(`\n",
    "- Sort: `df.orderBy(expr or col or col_name)`\n",
    "- Limit: `df.limit(n)`\n",
    "\n",
    "[back to top](#directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('csv').option('header', True).option('inferSchema', True).load(data_example_path)\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|(count > 100)|\n",
      "+-------------+\n",
      "|        false|\n",
      "|        false|\n",
      "|         true|\n",
      "+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this create a new table with a bool column whether the count > 100\n",
    "# this can not do filtering!\n",
    "df.selectExpr('count>100').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|             Russia|  161|\n",
      "|          Iceland|      United States|  181|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|             Russia|  161|\n",
      "|          Iceland|      United States|  181|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|             Russia|  161|\n",
      "|          Iceland|      United States|  181|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|             Russia|  161|\n",
      "|          Iceland|      United States|  181|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# below are equivalent \n",
    "df.where('count>100 and count<200').show(2)\n",
    "df.where('count>100').where('count<200').show(2)\n",
    "df.filter('count>100').filter('count<200').show(2)\n",
    "df.filter('count>100 and count<200').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|   dest_country_name|\n",
      "+--------------------+\n",
      "|             Germany|\n",
      "|Turks and Caicos ...|\n",
      "|              France|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique\n",
    "df.where('count>200').select('dest_country_name').distinct().show(3)\n",
    "\n",
    "# count the number of unique destinations that have count > 200\n",
    "df.where('count>200').select('dest_country_name').distinct().count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "174\n"
     ]
    }
   ],
   "source": [
    "# random sample \n",
    "withReplacement, seed, fraction = True, 5, 0.7\n",
    "print(df.count())\n",
    "print(df.sample(withReplacement, fraction, seed).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "# random split\n",
    "seed = 5\n",
    "par = [0.7, 0.3]  # if the ratio list does not add up to 1, the fractions will be normalized so that add up to 1!\n",
    "subdfs = df.randomSplit(par, seed)  # it return a list of dfs!\n",
    "print(subdfs[0].count())\n",
    "print(subdfs[1].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|              NYC|                EWR|    4|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType\n",
    "# Add new rows into df\n",
    "# usually, you might want to register each new df created by register it or make it as a view\n",
    "# so you can access it more dynamically. \n",
    "\n",
    "# prepare the new rows\n",
    "rows = [Row('NYC', 'EWR', 4),\n",
    "        Row('MIA', 'EWR', 201),\n",
    "        Row('HNL', 'MIA', 2)]\n",
    "\n",
    "# parallelize the rows\n",
    "# work on list of Row objects\n",
    "parallelizedRows = spark.sparkContext.parallelize(rows)\n",
    "\n",
    "# create a schema for it\n",
    "field_1 = StructField('DEST_COUNTRY_NAME', StringType(), True)\n",
    "field_2 = StructField('ORIGIN_COUNTRY_NAME', StringType(), True)\n",
    "field_3 = StructField('count', IntegerType(), False)\n",
    "\n",
    "schema = StructType([field_1, field_2, field_3])\n",
    "\n",
    "# create new DF\n",
    "newDF = spark.createDataFrame(parallelizedRows, schema)\n",
    "\n",
    "# append\n",
    "newDF = df.union(newDF)\n",
    "newDF.where(col('DEST_COUNTRY_NAME') == 'NYC').show()  # show "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Moldova|      United States|    1|\n",
      "|    United States|            Croatia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Moldova|      United States|    1|\n",
      "|    United States|            Croatia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sort - by default is ascending\n",
    "df.sort('count').show(2)  \n",
    "df.orderBy('count').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Partitions and Collect <a id='sec7'></a>\n",
    "\n",
    "Partition could improve the performance of operations when dataset is larger. You can call repartition on the df or one column of the df if the column will always be operated on.\n",
    "\n",
    "- Repartition and Coalesce \n",
    "    - `df.repartition(n)` use when n is larger than the current number of partitions\n",
    "    - `df.repartition(n, col(col_name))` when want to repartition specific column\n",
    "    - `df.rdd.getNumPartitions()` to check the current number of partitions\n",
    "    - `df.coalesce(n)` to combine the paritions\n",
    "    - **paritions and coalesce will incur shuffle of entire dataframe, could be slow!**\n",
    " \n",
    "- Collect\n",
    "\n",
    "For performance, it is recommeneded to sort within each partition before another set of operations\n",
    "- `df.sortWithinPartitions(expr or col or col_name)`\n",
    "\n",
    "\n",
    "Sometime you might want to pull the entire data from cluster (executors) to the driver (master). \n",
    "Functions such as **`show()` will pull a small portion of data (default 20 entries or depends on the param of `show()`) from executors to the driver memory** so that you can print it. \n",
    "\n",
    "Three functions that will incurr transfer data frame to driver memory\n",
    "- `df.show(n)` will take n entries to driver and prints out\n",
    "- `df.take(n)` will create a df on driver memory\n",
    "- `df.collect()` will collect the df to the driver memory\n",
    "\n",
    "[back to top](#directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Moldova|      United States|    1|\n",
      "|    United States|            Croatia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Algeria|      United States|    4|\n",
      "|           Angola|      United States|   15|\n",
      "|         Anguilla|      United States|   41|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "|           Canada|      United States|  8399|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc, asc\n",
    "\n",
    "# sort - specify order\n",
    "df.orderBy(col('count').desc()).show(2)\n",
    "df.orderBy(expr('count desc')).show(2)\n",
    "\n",
    "# sort - multiple\n",
    "df.orderBy(col('Dest_country_name').asc(), col('count').desc()).show(3)\n",
    "df.orderBy(col('count').desc(), col('Dest_country_name').asc()).show(3)  # different lexi order!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take k ~ return the first 5 rows as a new df\n",
    "df.limit(5).show()  # different than df.show(5)!!\n",
    "df.limit(5).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "|            Malta|      United States|    1|\n",
      "|    United States|          Gibraltar|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sort within partitions - for better performance\n",
    "# this actually sorted the whole list\n",
    "# because we only have one partition\n",
    "df.sortWithinPartitions('count').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check number of partitions\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repartition on df\n",
    "df.repartition(5).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repartition on column\n",
    "df.repartition(col('count')).rdd.getNumPartitions()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# default n is 200, can also specify it!\n",
    "df.repartition(5, col('count')).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use coalesce to combine the partitions\n",
    "partitioned = df.repartition(100)\n",
    "partitioned.coalesce(2).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1)]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# below three operations all inccur transfering data to driver memory\n",
    "\n",
    "df.take(2)  # this takes two rows into the driver memory\n",
    "df.show(2)  # this take two rows in memory and prints out the two rows\n",
    "df.limit(2).collect()  # this first create a 2 row data frame then collect it to the driver"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
