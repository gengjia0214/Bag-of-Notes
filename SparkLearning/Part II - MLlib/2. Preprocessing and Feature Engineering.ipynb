{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Learning Note - Preprocessing and Feature Engineering\n",
    "\n",
    "Jia Geng | gjia0214@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='directory'></a>\n",
    "\n",
    "## Directory\n",
    "\n",
    "- [Data Source](https://github.com/databricks/Spark-The-Definitive-Guide/tree/master/data/)\n",
    "- [1. Formating Modelings for Different Type of Tasks](#sec1)\n",
    "- [2. `pyspark.ml.feature`](#sec2)\n",
    "    - [2.1 Common Transformer for Feature Transformation](#sec2-1)\n",
    "    - [2.2 Transform Continuous Data](#sec2-2)\n",
    "    - [2.3 Transform Categorical Data](#sec2-3)\n",
    "    - [2.4 Transform Text Data](#sec2-4)\n",
    "    - [2.5 Feature Manipulation and Selection](#sec2-5)\n",
    "- [3. Advanced Topics](#sec3)\n",
    "    - [3.1 Persisting Transformer](#sec3-1)\n",
    "    - [3.2 Writing a Custom Transformer](#sec3-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://unknown40A5EF2BBD8A:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MLExample</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fec4c978e50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('MLExample').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the example data\n",
    "sale_path = '/home/jgeng/Documents/Git/SparkLearning/book_data/retail-data/by-day/*.csv' \n",
    "int_path = '/home/jgeng/Documents/Git/SparkLearning/book_data/simple-ml-integers'\n",
    "simple_path = '/home/jgeng/Documents/Git/SparkLearning/book_data/simple-ml'\n",
    "scale_path = '/home/jgeng/Documents/Git/SparkLearning/book_data/simple-ml-scaling'\n",
    "sales = spark.read.format('csv').option('header', True)\\\n",
    "                                .option('inferSchema', True)\\\n",
    "                                .load(sale_path).coalesce(5).where('Description is not null')\n",
    "fakeIntDF = spark.read.parquet(int_path)\n",
    "simpleDF = spark.read.json(simple_path)\n",
    "scaleDF = spark.read.parquet(scale_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|       Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084|RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
      "+---------+---------+------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----+----+----+\n",
      "|int1|int2|int3|\n",
      "+----+----+----+\n",
      "|   4|   5|   6|\n",
      "+----+----+----+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----+----+------+------------------+\n",
      "|color| lab|value1|            value2|\n",
      "+-----+----+------+------------------+\n",
      "|green|good|     1|14.386294994851129|\n",
      "+-----+----+------+------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+---+--------------+\n",
      "| id|      features|\n",
      "+---+--------------+\n",
      "|  0|[1.0,0.1,-1.0]|\n",
      "+---+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.show(1)\n",
    "sales.cache()\n",
    "fakeIntDF.show(1)\n",
    "simpleDF.show(1)\n",
    "scaleDF.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Formating Modelings for Different Type of Tasks <a id='sec1'></a>\n",
    "\n",
    "Classification and Regression\n",
    "- a Double Type column for label\n",
    "- a Vector[Double] column for features\n",
    "\n",
    "Recommendation\n",
    "- a column for user\n",
    "- a column for items (movies or books)\n",
    "- a column for rating\n",
    "\n",
    "Unsupervised Learning\n",
    "- a Vector (dense/sparse) for feature\n",
    "\n",
    "Graph Analysis\n",
    "- a DataFrame of vertices \n",
    "- a DataFrame of edges\n",
    "\n",
    "Two types of feature transformation\n",
    "- **Transfomer**: convert data in a way that is not affected by input data, e.g. `Tokenizer`\n",
    "    - all transformer and some estimator for preprocessing has a `setInputCol()` and `setOutputCol` method.\n",
    "    - transformer has a `transform()` to perform the transformation\n",
    "- **Estimator** for preprocessing: convert data in a way that is affected by input data, e.g. scaling, normalizaton, `StandardScaler` the transformatin affected by input value and distribution\n",
    "    - estimator need to be `fit()` on data first, then perform transforming using the fitted object\n",
    "    \n",
    "[back to top](#directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. pyspark.ml.feature <a id='sec2'></a>\n",
    "\n",
    "[back to top](#directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Common Transformer for Feature Transformation <a id='sec2-1'></a>\n",
    "- **High Level Transformers**\n",
    "    - RFormula: good for conventionally formatted data. No need to extract values from strings or manipulate them in any way. RFormula will automatically handle categorical input by performing one-hot encoding. Numberic and labels are converted to double.\n",
    "        - need a `.fit()` process\n",
    "    - check MLlib overview for example\n",
    "- **VectorAssembler**: Concate the features into one big vector. A tool that will be used (directly or indirectly) in nearly every pipline. \n",
    "    - Do transformation on each column then combine them using the assembler\n",
    "- **SQL Transformer** use SQL language to transform the data\n",
    "    - Most flexible build-in transformer\n",
    "    \n",
    "[back to top](#directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Transform Continuous Data  <a id='sec2-2'></a>\n",
    "\n",
    "Common transformation for continuous data including: **scaling** and **categorizing**.\n",
    "\n",
    "Categorizer:\n",
    "- **Bucketizer**: split conitnuous feature into buckets.\n",
    "    - need to specify the buckets with a list of splitting point (include the two boundary)\n",
    "    - use float('inf') or float('-inf') if needed \n",
    "- **QuantileDiscretizer**: use equally divided quantiles to discretinize the data\n",
    "    - need to fit on data to find the quantiles\n",
    " \n",
    "Scaler:\n",
    "- **Standard Scaler**: standardize the data. \n",
    "    - support standardize the data vectors. \n",
    "    - vector in spark is a list of different features\n",
    "    - standardization is conducted on the rows of data, instead of within each vector!!!\n",
    "- **MinMaxScaler**: 0 ~ 1\n",
    "- **MaxAbsScaler**: -1 ~ 1\n",
    "- **ElementwiseProduct**: scale each value within a vector by an arbitrary value\n",
    "    - use `pyspark.ml.linalg.Vectors` `Vectors.dense()` to declare the scaling vector\n",
    "    - `.dense()` must take float input, size must match\n",
    "    - since using arbitrary scales, no `.fit()` process\n",
    "- **Normalizer**: \n",
    "    - normalize by the lp norm of the vector (not by the norm of the feature!)\n",
    "\n",
    "[back to top](#directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------------------+--------+-------------------+---------+----------+--------------+--------------------+\n",
      "|InvoiceNo|StockCode|       Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|           Tokenized|\n",
      "+---------+---------+------------------+--------+-------------------+---------+----------+--------------+--------------------+\n",
      "|   580538|    23084|RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|[rabbit, night, l...|\n",
      "+---------+---------+------------------+--------+-------------------+---------+----------+--------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# tokenizer, can be applied directly on the data!!\n",
    "tkn = Tokenizer().setInputCol('Description').setOutputCol('Tokenized')\n",
    "tkn.transform(sales).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+-------------+\n",
      "|int1|int2|int3|    Assembled|\n",
      "+----+----+----+-------------+\n",
      "|   4|   5|   6|[4.0,5.0,6.0]|\n",
      "+----+----+----+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vca = VectorAssembler().setInputCols(['int1', 'int2', 'int3']).setOutputCol('Assembled')\n",
    "vca.transform(fakeIntDF).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "+---+\n",
      "only showing top 1 row\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = false)\n",
      "\n",
      "root\n",
      " |-- id: double (nullable = false)\n",
      "\n",
      "+---+------+\n",
      "| id|bucket|\n",
      "+---+------+\n",
      "|0.0|   0.0|\n",
      "|1.0|   1.0|\n",
      "|2.0|   1.0|\n",
      "|3.0|   2.0|\n",
      "|4.0|   2.0|\n",
      "+---+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# convert the integer to doulbe\n",
    "contDF = spark.range(20)\n",
    "contDF.show(1)\n",
    "contDF.printSchema()\n",
    "contDF = contDF.withColumn('id', expr('cast(id as double)'))\n",
    "contDF.printSchema()\n",
    "\n",
    "# bucketize\n",
    "splits = [float('-inf'), 1, 3, 10, 15, 20, float('inf')]\n",
    "bucketer = Bucketizer().setInputCol('id').setOutputCol('bucket').setSplits(splits)\n",
    "bucketer.transform(contDF).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|bucket|\n",
      "+---+------+\n",
      "|0.0|   1.0|\n",
      "|1.0|   2.0|\n",
      "+---+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "\n",
    "# Quantile Discretizer Need to be fit on data to find the quantiles\n",
    "qdt = QuantileDiscretizer().setInputCol('id').setOutputCol('bucket')\\\n",
    "                            .setNumBuckets(100).setRelativeError(0)\n",
    "qdt.fit(contDF).transform(contDF).show(2)  # if there are void buckets, will ignore them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+------------------------------------------------------------+\n",
      "|id |features      |standardized                                                |\n",
      "+---+--------------+------------------------------------------------------------+\n",
      "|0  |[1.0,0.1,-1.0]|[1.1952286093343936,0.02337622911060922,-0.5976143046671968]|\n",
      "|1  |[2.0,1.1,1.0] |[2.390457218668787,0.2571385202167014,0.5976143046671968]   |\n",
      "|0  |[1.0,0.1,-1.0]|[1.1952286093343936,0.02337622911060922,-0.5976143046671968]|\n",
      "|1  |[2.0,1.1,1.0] |[2.390457218668787,0.2571385202167014,0.5976143046671968]   |\n",
      "|1  |[3.0,10.1,3.0]|[3.5856858280031805,2.3609991401715313,1.7928429140015902]  |\n",
      "+---+--------------+------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# scaler need to learn the mean and std from the data\n",
    "# so there is a fit step!!!!\n",
    "ss = StandardScaler().setInputCol('features').setOutputCol('standardized')\n",
    "stds = ss.fit(scaleDF)\n",
    "\n",
    "# standardization is across rows NOT within each vector!!!\n",
    "stds.transform(scaleDF).show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-------------+\n",
      "|id |features      |scaled       |\n",
      "+---+--------------+-------------+\n",
      "|0  |[1.0,0.1,-1.0]|[0.0,0.0,0.0]|\n",
      "|1  |[2.0,1.1,1.0] |[0.5,0.1,0.5]|\n",
      "|0  |[1.0,0.1,-1.0]|[0.0,0.0,0.0]|\n",
      "|1  |[2.0,1.1,1.0] |[0.5,0.1,0.5]|\n",
      "|1  |[3.0,10.1,3.0]|[1.0,1.0,1.0]|\n",
      "+---+--------------+-------------+\n",
      "\n",
      "+---+--------------+-------------------------------------------------------------+\n",
      "|id |features      |scaled                                                       |\n",
      "+---+--------------+-------------------------------------------------------------+\n",
      "|0  |[1.0,0.1,-1.0]|[0.3333333333333333,0.009900990099009901,-0.3333333333333333]|\n",
      "|1  |[2.0,1.1,1.0] |[0.6666666666666666,0.10891089108910892,0.3333333333333333]  |\n",
      "|0  |[1.0,0.1,-1.0]|[0.3333333333333333,0.009900990099009901,-0.3333333333333333]|\n",
      "|1  |[2.0,1.1,1.0] |[0.6666666666666666,0.10891089108910892,0.3333333333333333]  |\n",
      "|1  |[3.0,10.1,3.0]|[1.0,1.0,1.0]                                                |\n",
      "+---+--------------+-------------------------------------------------------------+\n",
      "\n",
      "+---+--------------+--------------+\n",
      "|id |features      |scaled        |\n",
      "+---+--------------+--------------+\n",
      "|0  |[1.0,0.1,-1.0]|[1.0,0.2,-3.0]|\n",
      "|1  |[2.0,1.1,1.0] |[2.0,2.2,3.0] |\n",
      "|0  |[1.0,0.1,-1.0]|[1.0,0.2,-3.0]|\n",
      "|1  |[2.0,1.1,1.0] |[2.0,2.2,3.0] |\n",
      "|1  |[3.0,10.1,3.0]|[3.0,20.2,9.0]|\n",
      "+---+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler, MaxAbsScaler, ElementwiseProduct\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# min max scale -> 0 ~ 1\n",
    "mms = MinMaxScaler().setInputCol('features').setOutputCol('scaled')\n",
    "mms.fit(scaleDF).transform(scaleDF).show(20, False)\n",
    "\n",
    "# scale -> -1 ~ 1\n",
    "ma = MaxAbsScaler().setInputCol('features').setOutputCol('scaled')\n",
    "ma.fit(scaleDF).transform(scaleDF).show(20, False)\n",
    "\n",
    "# arbitrary scale\n",
    "# must delace a vector\n",
    "# must use float, size must match\n",
    "scales = Vectors.dense(1.0, 2.0, 3.0)\n",
    "eps = ElementwiseProduct().setInputCol('features').setOutputCol('scaled').setScalingVec(scales)\n",
    "eps.transform(scaleDF).show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---------------------------------------------------------------+\n",
      "|id |features      |Normalizer_1679abac28bc__output                                |\n",
      "+---+--------------+---------------------------------------------------------------+\n",
      "|0  |[1.0,0.1,-1.0]|[0.47619047619047616,0.047619047619047616,-0.47619047619047616]|\n",
      "|1  |[2.0,1.1,1.0] |[0.48780487804878053,0.26829268292682934,0.24390243902439027]  |\n",
      "|0  |[1.0,0.1,-1.0]|[0.47619047619047616,0.047619047619047616,-0.47619047619047616]|\n",
      "|1  |[2.0,1.1,1.0] |[0.48780487804878053,0.26829268292682934,0.24390243902439027]  |\n",
      "|1  |[3.0,10.1,3.0]|[0.18633540372670807,0.6273291925465838,0.18633540372670807]   |\n",
      "+---+--------------+---------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "# normalize each vector by the L-p norm\n",
    "# E.g. l1 norm for first vector 1 + 1 + 0.1 = 2.1  ==> 1/2.1 = 0.476\n",
    "nm = Normalizer().setInputCol('features').setP(1)\n",
    "nm.transform(scaleDF).show(30, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Transform Categorical Data <a id='sec2-3'></a>\n",
    "\n",
    "The most common task for categorical data is indexing: converts a categorical variable in a column to numerical one that can be plug into machine learning algorithm. Some time the input need to be tokenized before such transformation.\n",
    "\n",
    "**Categorical Encoding**\n",
    "\n",
    "- **StringIndexer**: maps strings to different numerical ids\n",
    "    - work on the whole string not each word\n",
    "            - for each word, use `Tokenizer`\n",
    "    - need to `.fit()` on data\n",
    "    - also works on non-string column\n",
    "    - **StringIndexer can not handle unseen data by default.** The other option is to skip the row with an unseen data.\n",
    "    - encode with certain order \n",
    "\n",
    "- **IndexToString**: convert the ids back to strings.\n",
    "    - no need to specify the string column or the matching, spark will handle all the metadata\n",
    "    - if you do need to specify the labels, do this via `.setLabels()`\n",
    "\n",
    "- **VectorIndexer**: automatic detect the numerical categorical data to 0-based cateorical data.\n",
    "    - need to fit on data\n",
    "    - need to `setMaxCategories()`\n",
    "        - E.g. if set to 2, it will detect all features that have 2 or less than 2 unqiue values then convert it to 0-based indexes\n",
    "    - be careful if you have features that are not categorical but does not have much unique values\n",
    "    \n",
    "**One-Hot Encoding**\n",
    "- **OneHotEncoder**: one hot does not introduce the numerical difference between cats\n",
    "    - **one hot encoder only works on the numerical data, usually need to convert the data using StringEncoder, then apply one hot on it**\n",
    "    - thus, one hot encoder does not need the fit process\n",
    "    - spark use sparse matrix for one hot \n",
    "    - by default it will drop the last cat\n",
    "    \n",
    "[back to top](#directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+-------+\n",
      "|color| lab|value1|            value2|labelID|\n",
      "+-----+----+------+------------------+-------+\n",
      "|green|good|     1|14.386294994851129|    1.0|\n",
      "| blue| bad|     8|14.386294994851129|    0.0|\n",
      "+-----+----+------+------------------+-------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----+----+------+------------------+-------+\n",
      "|color| lab|value1|            value2|labelID|\n",
      "+-----+----+------+------------------+-------+\n",
      "|green|good|     1|14.386294994851129|    2.0|\n",
      "| blue| bad|     8|14.386294994851129|    4.0|\n",
      "| blue| bad|    12|14.386294994851129|    0.0|\n",
      "|green|good|    15| 38.97187133755819|    5.0|\n",
      "|green|good|    12|14.386294994851129|    0.0|\n",
      "+-----+----+------+------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# automatically encode the string into numerical ids\n",
    "stdIdxer = StringIndexer().setInputCol('lab').setOutputCol('labelID')\n",
    "encoded = stdIdxer.fit(simpleDF).transform(simpleDF)\n",
    "encoded.show(2)\n",
    "\n",
    "# also work with numerical column\n",
    "stdIdxer = StringIndexer().setInputCol('value1').setOutputCol('labelID')\n",
    "stdIdxer.fit(simpleDF).transform(simpleDF).show(5)\n",
    "\n",
    "\n",
    "# if want to skip the unseen data\n",
    "stdIdxer = stdIdxer.setHandleInvalid('skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+-------+----------------------------------+\n",
      "|color| lab|value1|            value2|labelID|IndexToString_8c624bf87972__output|\n",
      "+-----+----+------+------------------+-------+----------------------------------+\n",
      "|green|good|     1|14.386294994851129|    1.0|                              good|\n",
      "| blue| bad|     8|14.386294994851129|    0.0|                               bad|\n",
      "+-----+----+------+------------------+-------+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----+----+------+------------------+-------+----------------------------------+\n",
      "|color| lab|value1|            value2|labelID|IndexToString_46daa60f5f9e__output|\n",
      "+-----+----+------+------------------+-------+----------------------------------+\n",
      "|green|good|     1|14.386294994851129|    1.0|                             Great|\n",
      "| blue| bad|     8|14.386294994851129|    0.0|                          Terrible|\n",
      "+-----+----+------+------------------+-------+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IndexToString\n",
    "\n",
    "# covert back, original mapping was handled by spark\n",
    "idx2str = IndexToString().setInputCol('labelID')\n",
    "idx2str.transform(encoded).show(2)\n",
    "\n",
    "# covert back, original mapping was handled by spark\n",
    "idx2str = IndexToString().setInputCol('labelID').setLabels(['Terrible', 'Great'])\n",
    "idx2str.transform(encoded).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|     features|label|\n",
      "+-------------+-----+\n",
      "|[1.0,2.0,3.0]|    1|\n",
      "|[2.0,5.0,6.0]|    2|\n",
      "|[1.0,8.0,9.0]|    3|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "sample = spark.createDataFrame([([Vectors.dense([1, 2, 3]), 1]),\n",
    "                               ([Vectors.dense([2, 5, 6]), 2]),\n",
    "                               ([Vectors.dense([1, 8, 9]), 3])], ['features', 'label'])\n",
    "sample.show()\n",
    "\n",
    "# set up the vector indexer and max cats\n",
    "vidxerA = VectorIndexer().setInputCol('features').setMaxCategories(2)  # 2 cats \n",
    "vidxerB = VectorIndexer().setInputCol('features').setMaxCategories(3)  # 3 cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+----------------------------------+\n",
      "|     features|label|VectorIndexer_4f7942a7eb42__output|\n",
      "+-------------+-----+----------------------------------+\n",
      "|[1.0,2.0,3.0]|    1|                     [0.0,2.0,3.0]|\n",
      "|[2.0,5.0,6.0]|    2|                     [1.0,5.0,6.0]|\n",
      "|[1.0,8.0,9.0]|    3|                     [0.0,8.0,9.0]|\n",
      "+-------------+-----+----------------------------------+\n",
      "\n",
      "+-------------+-----+----------------------------------+\n",
      "|     features|label|VectorIndexer_6daa1678ddbc__output|\n",
      "+-------------+-----+----------------------------------+\n",
      "|[1.0,2.0,3.0]|    1|                     [0.0,0.0,0.0]|\n",
      "|[2.0,5.0,6.0]|    2|                     [1.0,1.0,1.0]|\n",
      "|[1.0,8.0,9.0]|    3|                     [0.0,2.0,2.0]|\n",
      "+-------------+-----+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A max cat = 2 so only covert the first feature\n",
    "vidxerA.fit(sample).transform(sample).show()\n",
    "\n",
    "# B max cat = 3 so converts all features to 0-based index\n",
    "vidxerB.fit(sample).transform(sample).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+-------+-------------+\n",
      "|color| lab|value1|            value2|colorId|       onehot|\n",
      "+-----+----+------+------------------+-------+-------------+\n",
      "|green|good|     1|14.386294994851129|    1.0|(3,[1],[1.0])|\n",
      "| blue| bad|     8|14.386294994851129|    2.0|(3,[2],[1.0])|\n",
      "| blue| bad|    12|14.386294994851129|    2.0|(3,[2],[1.0])|\n",
      "|green|good|    15| 38.97187133755819|    1.0|(3,[1],[1.0])|\n",
      "|green|good|    12|14.386294994851129|    1.0|(3,[1],[1.0])|\n",
      "+-----+----+------+------------------+-------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "# label encoding first\n",
    "strIdxer = StringIndexer().setInputCol('color').setOutputCol('colorId')\n",
    "encoded = strIdxer.fit(simpleDF).transform(simpleDF)\n",
    "\n",
    "# one-hot encoding on the label encoded data\n",
    "# one-hot only works on numerical data\n",
    "# must transform to numerical indexes first\n",
    "onehot = OneHotEncoder().setInputCol('colorId').setOutputCol('onehot').setDropLast(False)\n",
    "onehot.transform(encoded).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Transform Text Data <a id='sec2-4'></a>\n",
    "\n",
    "**Tokenizer**\n",
    "\n",
    "- take a string or words seperated by space, covert them into an **array** of words\n",
    "\n",
    "**RegexTokenizer**\n",
    "- use regex expression, only the substring that match with the expression will be put into the array\n",
    "\n",
    "**StopWordsRemover**\n",
    "- remove the common words\n",
    "- support diffrent languages via `loadDefaultsStopWords()`\n",
    "\n",
    "**N Gram**\n",
    "- for each document (row) combine all N consecutive words together\n",
    "\n",
    "**CountVectorizer**\n",
    "- treats each row as a document every word as a term and the total collection of terms as vocabulary.\n",
    "- `fit()` will scan the words in all documents and count\n",
    "- `transform()` will output a sparse vector with the terms that occur in that row\n",
    "    - sparse vector `[voca_size, [term1_idx, term2_idx, ...], [term1_count, ...]]`\n",
    "- many options\n",
    "     - `minTF` for minimum term frequency to be included in vocabulary\n",
    "     - `minDF` for minimum document appreance to be included in vocabulary\n",
    "     - `vocabSize` for maximum vocabulary size\n",
    "     - `setBinary(True)` if only want to use whether a word exisits in a document, False for output the frequency of the term\n",
    "     \n",
    "**HashingTF & IDF - TFID**\n",
    "- `HashingTF` is similar to `CountVectorizer` but is ireversible. It does NOT have `fit()` as it does not provide the term index information. it provide term frequence info.\n",
    "- `IDF` provide the document frequency info. \n",
    "    - `IDF` requires `fit()` step because it build term indexes\n",
    "- TF-IDF is also reprented by sparse vector.\n",
    "\n",
    "**Word2Vec**\n",
    "- vectorize words by the relationships between words based on their semantics.\n",
    "- work best when the input is free-form text in the form of tokens\n",
    "- need to specify the vector size when construct `Word2Vec`\n",
    "- need a fit step, many other configurations are supported\n",
    "\n",
    "[back to top](#directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+\n",
      "|InvoiceNo|        Description|\n",
      "+---------+-------------------+\n",
      "|   580538| RABBIT NIGHT LIGHT|\n",
      "|   580538|DOUGHNUT LIP GLOSS |\n",
      "+---------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "descDF = sales.select('InvoiceNo', 'Description')\n",
    "descDF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+--------------------+\n",
      "|InvoiceNo|        Description|              Tokens|\n",
      "+---------+-------------------+--------------------+\n",
      "|   580538| RABBIT NIGHT LIGHT|[rabbit, night, l...|\n",
      "|   580538|DOUGHNUT LIP GLOSS |[doughnut, lip, g...|\n",
      "+---------+-------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# it will convert everything to lower case!\n",
    "tokenizer = Tokenizer().setInputCol('Description').setOutputCol('Tokens')\n",
    "tokenizer.transform(descDF).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+--------------------+\n",
      "|InvoiceNo|        Description|              Tokens|\n",
      "+---------+-------------------+--------------------+\n",
      "|   580538| RABBIT NIGHT LIGHT|[rabbit, night, l...|\n",
      "|   580538|DOUGHNUT LIP GLOSS |[doughnut, lip, g...|\n",
      "+---------+-------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---------+-------------------+--------------------+\n",
      "|InvoiceNo|        Description|              Tokens|\n",
      "+---------+-------------------+--------------------+\n",
      "|   580538| RABBIT NIGHT LIGHT|[r, a, b, b, i, t...|\n",
      "|   580538|DOUGHNUT LIP GLOSS |[d, o, u, g, h, n...|\n",
      "+---------+-------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---------+-------------------+---------+\n",
      "|InvoiceNo|        Description|    Token|\n",
      "+---------+-------------------+---------+\n",
      "|   580538| RABBIT NIGHT LIGHT|[i, i, i]|\n",
      "|   580538|DOUGHNUT LIP GLOSS |      [i]|\n",
      "+---------+-------------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "\n",
    "# this is identical to tokenizer\n",
    "tokenizer = RegexTokenizer().setInputCol('Description').setOutputCol('Tokens')\n",
    "tokenizer.transform(descDF).show(2)\n",
    "\n",
    "# use setPattern to set up the delimiter\n",
    "# this is the default behavior\n",
    "tokenizer = RegexTokenizer().setInputCol('Description').setOutputCol('Tokens').setPattern('')\n",
    "tokenizer.transform(descDF).show(2)\n",
    "\n",
    "# if setGap(False)\n",
    "# then setPattern() will be used for extracting words that match with the pattern\n",
    "tokenizer = RegexTokenizer().setInputCol('Description').setOutputCol('Token')\\\n",
    "                            .setGaps(False).setPattern('i')\n",
    "tokenizer.transform(descDF).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopwords: ['i', 'me', 'my', 'myself', 'we']\n",
      "+---------+--------------------+--------------------+-------------------------------------+\n",
      "|InvoiceNo|         Description|              Tokens|StopWordsRemover_1f4efa701d1b__output|\n",
      "+---------+--------------------+--------------------+-------------------------------------+\n",
      "|   580538|  RABBIT NIGHT LIGHT|[rabbit, night, l...|                 [rabbit, night, l...|\n",
      "|   580538| DOUGHNUT LIP GLOSS |[doughnut, lip, g...|                 [doughnut, lip, g...|\n",
      "|   580538|12 MESSAGE CARDS ...|[12, message, car...|                 [12, message, car...|\n",
      "+---------+--------------------+--------------------+-------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# prepare the remover: put stop words in list -> set it up\n",
    "stopwords = StopWordsRemover.loadDefaultStopWords('english')\n",
    "print('stopwords:', stopwords[:5])  # append anything else to the list \n",
    "swr = StopWordsRemover().setInputCol('Tokens').setStopWords(stopwords)\n",
    "\n",
    "# stop word remover can only work on tokenized column (array of words)\n",
    "tokenized = Tokenizer().setInputCol('Description').setOutputCol('Tokens').transform(descDF)\n",
    "swr.transform(tokenized).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+-------------------------------------+\n",
      "|InvoiceNo|         Description|              Tokens|StopWordsRemover_1f4efa701d1b__output|\n",
      "+---------+--------------------+--------------------+-------------------------------------+\n",
      "|   580538|  RABBIT NIGHT LIGHT|[rabbit, night, l...|                 [rabbit, night, l...|\n",
      "|   580538| DOUGHNUT LIP GLOSS |[doughnut, lip, g...|                 [doughnut, lip, g...|\n",
      "|   580538|12 MESSAGE CARDS ...|[12, message, car...|                 [12, message, car...|\n",
      "+---------+--------------------+--------------------+-------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-------------------------------------+-------------------------------------------------------+\n",
      "|Tokens                               |ngram                                                  |\n",
      "+-------------------------------------+-------------------------------------------------------+\n",
      "|[rabbit, night, light]               |[rabbit night, night light]                            |\n",
      "|[doughnut, lip, gloss]               |[doughnut lip, lip gloss]                              |\n",
      "|[12, message, cards, with, envelopes]|[12 message, message cards, cards with, with envelopes]|\n",
      "+-------------------------------------+-------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# stop word remover can only work on tokenized column (array of words)\n",
    "tokenized = Tokenizer().setInputCol('Description').setOutputCol('Tokens').transform(descDF)\n",
    "swr.transform(tokenized).show(3)\n",
    "\n",
    "# ngram only work with tokenized column\n",
    "ngramer = NGram().setInputCol('Tokens').setOutputCol('ngram').setN(2)\n",
    "ngramer.transform(tokenized).select('Tokens', 'ngram').show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+-------------------------------------+\n",
      "|InvoiceNo|         Description|              Tokens|StopWordsRemover_1f4efa701d1b__output|\n",
      "+---------+--------------------+--------------------+-------------------------------------+\n",
      "|   580538|  RABBIT NIGHT LIGHT|[rabbit, night, l...|                 [rabbit, night, l...|\n",
      "|   580538| DOUGHNUT LIP GLOSS |[doughnut, lip, g...|                 [doughnut, lip, g...|\n",
      "|   580538|12 MESSAGE CARDS ...|[12, message, car...|                 [12, message, car...|\n",
      "+---------+--------------------+--------------------+-------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---------+-------------------------------+-------------------------------------+------------------------------------------------+\n",
      "|InvoiceNo|Description                    |Tokens                               |Counts                                          |\n",
      "+---------+-------------------------------+-------------------------------------+------------------------------------------------+\n",
      "|580538   |RABBIT NIGHT LIGHT             |[rabbit, night, light]               |(2198,[150,185,212],[1.0,1.0,1.0])              |\n",
      "|580538   |DOUGHNUT LIP GLOSS             |[doughnut, lip, gloss]               |(2198,[462,463,492],[1.0,1.0,1.0])              |\n",
      "|580538   |12 MESSAGE CARDS WITH ENVELOPES|[12, message, cards, with, envelopes]|(2198,[35,41,166,782,943],[1.0,1.0,1.0,1.0,1.0])|\n",
      "+---------+-------------------------------+-------------------------------------+------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# count vectorizer only work on tokenized column (array of words)\n",
    "tokenized = Tokenizer().setInputCol('Description').setOutputCol('Tokens').transform(descDF)\n",
    "swr.transform(tokenized).show(3)\n",
    "\n",
    "# count the token column, need a fit step\n",
    "# support many settings such as min/maxDF, binary (only return whether term showed up), etc\n",
    "# in sparse vector, the second items is the term idx\n",
    "# count vectorizer will scan all data and build a vocabulary so each term will have a unique ID\n",
    "counter = CountVectorizer().setInputCol('Tokens').setOutputCol('Counts').setMinDF(5)\n",
    "counter.fit(tokenized).transform(tokenized).show(3, False)  # result in sparse vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+-------------------------------------+\n",
      "|InvoiceNo|         Description|              Tokens|StopWordsRemover_1f4efa701d1b__output|\n",
      "+---------+--------------------+--------------------+-------------------------------------+\n",
      "|   580539|GINGHAM HEART  DO...|[gingham, heart, ...|                 [gingham, heart, ...|\n",
      "|   580541|RED FLORAL FELTCR...|[red, floral, fel...|                 [red, floral, fel...|\n",
      "|   580543|ALARM CLOCK BAKEL...|[alarm, clock, ba...|                 [alarm, clock, ba...|\n",
      "+---------+--------------------+--------------------+-------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---------+--------------------+--------------------+--------------------+\n",
      "|InvoiceNo|         Description|              Tokens|              Hashed|\n",
      "+---------+--------------------+--------------------+--------------------+\n",
      "|   580539|GINGHAM HEART  DO...|[gingham, heart, ...|(262144,[47362,10...|\n",
      "|   580541|RED FLORAL FELTCR...|[red, floral, fel...|(262144,[31197,17...|\n",
      "|   580543|ALARM CLOCK BAKEL...|[alarm, clock, ba...|(262144,[3028,157...|\n",
      "+---------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---------+---------------------------+---------------------------------+------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------+\n",
      "|InvoiceNo|Description                |Tokens                           |Hashed                                                            |idf                                                                                                                         |\n",
      "+---------+---------------------------+---------------------------------+------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------+\n",
      "|580539   |GINGHAM HEART  DOORSTOP RED|[gingham, heart, , doorstop, red]|(262144,[47362,102296,180290,195459,249180],[1.0,1.0,1.0,1.0,1.0])|(262144,[47362,102296,180290,195459,249180],[3.528623998496698,2.9673076262843088,5.153268286376063,0.0,3.4529287244224034])|\n",
      "+---------+---------------------------+---------------------------------+------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.sql.functions import array_contains, round\n",
    "\n",
    "# only work on tokenized column (array of words)\n",
    "tokenized = Tokenizer().setInputCol('Description').setOutputCol('Tokens').transform(descDF)\n",
    "\n",
    "# only keep the rows that contains 'red'\n",
    "tokenized = tokenized.where(array_contains(col('Tokens'), 'red'))\n",
    "swr.transform(tokenized).show(3)\n",
    "\n",
    "# construct a hashingTF, simular to the CountVectorizer but it does not build the vocabulary map\n",
    "# so each word has an id with it, same term could have different id\n",
    "# e.g. the hashed column sparse vector shows there are ~ 26k vocabularies --> 26k words\n",
    "hasher = HashingTF().setInputCol('Tokens').setOutputCol('Hashed')\n",
    "hashed = hasher.transform(tokenized)\n",
    "hashed.show(3)\n",
    "\n",
    "# the idf column is a sparse vector: total vocabulary size, hash, weights\n",
    "idfer = IDF().setInputCol('Hashed').setOutputCol('idf')\n",
    "idfDF = idfer.fit(hashed).transform(hashed)\n",
    "idfDF.show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+\n",
      "|text                                      |\n",
      "+------------------------------------------+\n",
      "|[Hi,, I, heard, about, spark]             |\n",
      "|[I, wish, java, could, use, case, classes]|\n",
      "|[Logistic, regression, models, are, neat] |\n",
      "+------------------------------------------+\n",
      "\n",
      "root\n",
      " |-- text: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "docDF = spark.createDataFrame([\n",
    "    ('Hi, I heard about spark'.split(' '),),\n",
    "    ('I wish java could use case classes'.split(' '),),\n",
    "    ('Logistic regression models are neat'.split(' '),)\n",
    "], ['text'])  # must include the , for each row!!!\n",
    "\n",
    "docDF.show(5, False)\n",
    "docDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+-----------------------------------------------------------------+\n",
      "|text                                      |vector                                                           |\n",
      "+------------------------------------------+-----------------------------------------------------------------+\n",
      "|[Hi,, I, heard, about, spark]             |[-0.07255996316671372,0.029591199429705742,-0.0400250225327909]  |\n",
      "|[I, wish, java, could, use, case, classes]|[0.041278247854539325,-0.05718414538672992,0.01688022825068661]  |\n",
      "|[Logistic, regression, models, are, neat] |[-0.001246955245733261,0.09623391311615706,-0.009747498854994775]|\n",
      "+------------------------------------------+-----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2Vec(vectorSize=3, minCount=0).setInputCol('text').setOutputCol('vector')\n",
    "w2v.fit(docDF).transform(docDF).show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Feature Manipulation and Selection <a id='sec2-5'></a>\n",
    "\n",
    "**PCA**\n",
    "- simply specifying the features column and K components\n",
    "- fit and transform\n",
    "\n",
    "**Interaction**\n",
    "- Use `RFormula` is the easiest way and supported by pyspark\n",
    "- `Interaction` also allows user to create interactions manually (**2.2 only available for Scala**)\n",
    "\n",
    "**PolynomialExpansion**\n",
    "- feature expantion\n",
    "- set up the features column and the degree\n",
    "- no fit stage\n",
    "\n",
    "**ChiSqSelector**\n",
    "- https://towardsdatascience.com/chi-square-test-for-feature-selection-in-machine-learning-206b1f0b8223\n",
    "- leverage statistical test (chi square test) to find features that are not independent from the label, and drop the uncorrelated feature.\n",
    "- selection methods\n",
    "    - `numTopFeatures`: select n features by the order of p values\n",
    "    - `percentile`: select a porportion of the input features\n",
    "    - `fpr`: select features by an arbitrary cut off p-value\n",
    "\n",
    "[back to top](#directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- PCs: vector (nullable = true)\n",
      "\n",
      "+---+--------------+------------------------------------------+\n",
      "|id |features      |PCs                                       |\n",
      "+---+--------------+------------------------------------------+\n",
      "|0  |[1.0,0.1,-1.0]|[0.07137194992484153,-0.45266548881478463]|\n",
      "|1  |[2.0,1.1,1.0] |[-1.6804946984073725,1.2593401322219144]  |\n",
      "|0  |[1.0,0.1,-1.0]|[0.07137194992484153,-0.45266548881478463]|\n",
      "|1  |[2.0,1.1,1.0] |[-1.6804946984073725,1.2593401322219144]  |\n",
      "|1  |[3.0,10.1,3.0]|[-10.872398139848944,0.030962697060149758]|\n",
      "+---+--------------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "# pca usually works on the features column (the column that combines all features together) \n",
    "pcaer = PCA().setInputCol('features').setOutputCol('PCs').setK(2)  # k need to be smaller than input k\n",
    "reduced = pcaer.fit(scaleDF).transform(scaleDF)\n",
    "reduced.printSchema()\n",
    "reduced.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------+\n",
      "|expanded                                                                           |\n",
      "+-----------------------------------------------------------------------------------+\n",
      "|[1.0,1.0,0.1,0.1,0.010000000000000002,-1.0,-1.0,-0.1,1.0]                          |\n",
      "|[2.0,4.0,1.1,2.2,1.2100000000000002,1.0,2.0,1.1,1.0]                               |\n",
      "|[1.0,1.0,0.1,0.1,0.010000000000000002,-1.0,-1.0,-0.1,1.0]                          |\n",
      "|[2.0,4.0,1.1,2.2,1.2100000000000002,1.0,2.0,1.1,1.0]                               |\n",
      "|[3.0,9.0,10.1,30.299999999999997,102.00999999999999,3.0,9.0,30.299999999999997,9.0]|\n",
      "+-----------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PolynomialExpansion\n",
    "\n",
    "expansioner = PolynomialExpansion().setInputCol('features').setOutputCol('expanded').setDegree(2)\n",
    "expansioner.transform(scaleDF).select('expanded').show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|              tokens|CustomerID|\n",
      "+--------------------+----------+\n",
      "|[rabbit, night, l...|   14075.0|\n",
      "+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------------------+----------+-------------------------------+\n",
      "|tokens                |CustomerID|vec                            |\n",
      "+----------------------+----------+-------------------------------+\n",
      "|[rabbit, night, light]|14075.0   |(233,[70,83,155],[1.0,1.0,1.0])|\n",
      "+----------------------+----------+-------------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-------------------------------------+----------+--------------------------------------------+----------------------------------+\n",
      "|tokens                               |CustomerID|vec                                         |ChiSqSelector_c4efd7311477__output|\n",
      "+-------------------------------------+----------+--------------------------------------------+----------------------------------+\n",
      "|[rabbit, night, light]               |14075.0   |(233,[70,83,155],[1.0,1.0,1.0])             |(50,[27,32,41],[1.0,1.0,1.0])     |\n",
      "|[doughnut, lip, gloss]               |14075.0   |(233,[193,200,217],[1.0,1.0,1.0])           |(50,[47,48,49],[1.0,1.0,1.0])     |\n",
      "|[12, message, cards, with, envelopes]|14075.0   |(233,[13,28,43,56,77],[1.0,1.0,1.0,1.0,1.0])|(50,[17,22,30],[1.0,1.0,1.0])     |\n",
      "|[blue, harmonica, in, box]           |14075.0   |(233,[15,62,88,189],[1.0,1.0,1.0,1.0])      |(50,[24,33,46],[1.0,1.0,1.0])     |\n",
      "|[gumball, coat, rack]                |14075.0   |(233,[49,63,71],[1.0,1.0,1.0])              |(50,[19,25,28],[1.0,1.0,1.0])     |\n",
      "|[skulls, , water, transfer, tattoos] |14075.0   |(233,[3,9,17,97,183],[1.0,1.0,1.0,1.0,1.0]) |(50,[3,36,45],[1.0,1.0,1.0])      |\n",
      "|[feltcraft, girl, amelie, kit]       |14075.0   |(233,[4,32,166,179],[1.0,1.0,1.0,1.0])      |(50,[4,43,44],[1.0,1.0,1.0])      |\n",
      "|[camouflage, led, torch]             |14075.0   |(233,[92,103,159],[1.0,1.0,1.0])            |(50,[34,39,42],[1.0,1.0,1.0])     |\n",
      "|[white, skull, hot, water, bottle]   |18180.0   |(233,[3,6,8,105,228],[1.0,1.0,1.0,1.0,1.0]) |(50,[3,6,7],[1.0,1.0,1.0])        |\n",
      "|[english, rose, hot, water, bottle]  |18180.0   |(233,[3,6,8,42,55],[1.0,1.0,1.0,1.0,1.0])   |(50,[3,6,7,21],[1.0,1.0,1.0,1.0]) |\n",
      "+-------------------------------------+----------+--------------------------------------------+----------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml.feature import Tokenizer, CountVectorizer\n",
    "\n",
    "# tokenize input and fit a counter to vectorize the input\n",
    "tokenizer = Tokenizer().setInputCol('Description').setOutputCol('tokens')\n",
    "tokenized = tokenizer.transform(sales.limit(100)).select('tokens', 'CustomerID').where('CustomerID is not null')\n",
    "tokenized.show(1)\n",
    "cv = CountVectorizer().setInputCol('tokens').setOutputCol('vec')\n",
    "cvDF = cv.fit(tokenized).transform(tokenized)\n",
    "cvDF.show(1, False)\n",
    "\n",
    "# set up the chi square selector and fit, transform it\n",
    "# this takes a long time\n",
    "selector = ChiSqSelector().setFeaturesCol('vec').setLabelCol('CustomerID').setNumTopFeatures(50)\n",
    "selector.fit(cvDF).transform(cvDF).show(10, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Topic <a id='sec3'></a>\n",
    "\n",
    "### 3.1 Persisting Transformer <a id='sec3-1'></a>\n",
    "\n",
    "Some estimator require long fitting time, e.g. PCA. The estimator can be write on disk and load when needed to reduce the time spent on fitting.\n",
    "\n",
    "### 3.2 Writing a Custom Transformer <a id='sec3-2'></a>\n",
    " \n",
    "Most of the time, use the SQLTransformer which provides the flexibility to transform the data. Sometime, need to wrap some build-in methods and implement the transformer from scratch. But using Python will cause lots of overhead.\n",
    "\n",
    "[back to top](#directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
